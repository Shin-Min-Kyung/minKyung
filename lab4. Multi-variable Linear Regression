import tensorflow as tf #tensorflow 함수 부르기
import numpy as np #numpy함수 부르기

tf.enable_eager_execution()
tf.__version__ #버전 확인하기

tf.set_random_seed(0)  # for reproducibility #씨드 초기화

x1_data = [1, 0, 3, 0, 5] #X1값의 데이터
x2_data = [0, 2, 0, 4, 0] #X2값의 데이터
y_data  = [1, 2, 3, 4, 5] #y값의 데이터

W1 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))# assign은 뺀 값을 다시 W1에 할당한다. 값을 업데이트 시킨다.
W2 = tf.Variable(tf.random_uniform([1], -10.0, 10.0))# assign은 뺀 값을 다시 W2에 할당한다. 값을 업데이트 시킨다.
b  = tf.Variable(tf.random_uniform([1], -10.0, 10.0))# assign은 뺀 값을 다시 b에 할당한다. 값을 업데이트 시킨다.

learning_rate = tf.Variable(0.001)#러닝메이트는 grad의 값을 얼 만큼 반영할 것인지 결정한다.

for i in range(1000+1):#1000+1번 수행하게 한다.
    with tf.GradientTape() as tape: #경사하강법을 구현하는 방법, 변수를 tape에 기록한다.
        hypothesis = W1 * x1_data + W2 * x2_data + b #나의 가설 함수
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))#단순화 된 cost함수
    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])#기울기(개별 미분값 W1, W2, b)를 구해서 grad로 구현한다. 순서대로 구현된다.
    W1.assign_sub(learning_rate * W1_grad)# assign은 뺀 값을 다시 W1에 할당한다. 값을 업데이트 시킨다.
    W2.assign_sub(learning_rate * W2_grad)# assign은 뺀 값을 다시 W2에 할당한다. 값을 업데이트 시킨다.
    b.assign_sub(learning_rate * b_grad)# assign은 뺀 값을 다시 b에 할당한다. 값을 업데이트 시킨다.

    if i % 50 == 0:# i값이 50의 배수가 될 때마다 프린트한다.
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))#중간중간 50배수가 되는 값 프린트 한다.
  
 x_data = [[1., 0., 3., 0., 5.], [0., 2., 0., 4., 0.]] #X값의 데이터
y_data  = [1, 2, 3, 4, 5]#y값의 데이터

W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))# assign은 뺀 값을 다시 W에 할당한다. 값을 업데이트 시킨다.
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))# assign은 뺀 값을 다시 b에 할당한다. 값을 업데이트 시킨다.

learning_rate = tf.Variable(0.001) #러닝메이트는 grad의 값을 얼 만큼 반영할 것인지 결정한다.

for i in range(1000+1):#1000+1번 수행하게 한다.
    with tf.GradientTape() as tape:#경사하강법을 구현하는 방법, 변수를 tape에 기록한다.
        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)#나의 가설 함수
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))#단순화 된 cost함수
        W_grad, b_grad = tape.gradient(cost, [W, b]) #기울기(개별 미분값 W, b)를 구해서 grad로 구현한다. 순서대로 구현된다.
        W.assign_sub(learning_rate * W_grad)# assign은 뺀 값을 다시 W에 할당한다. 값을 업데이트 시킨다.
        b.assign_sub(learning_rate * b_grad)# assign은 뺀 값을 다시 b에 할당한다. 값을 업데이트 시킨다.
    
    if i % 50 == 0:#i값이 50의 배수가 될 때마다 프린트한다.
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))#중간중간 50배수가 되는 값 프린트 한다.
            
import tensorflow as tf #tensorflow 함수 부르기

# 앞의 코드에서 bias(b)를 행렬에 추가
x_data = [
    [1., 1., 1., 1., 1.], # bias(b)
    [1., 0., 3., 0., 5.], 
    [0., 2., 0., 4., 0.] #X값의 데이터
]
y_data  = [1, 2, 3, 4, 5] #y값의 데이터

W = tf.Variable(tf.random_uniform([1, 3], -1.0, 1.0)) # [1, 3]으로 변경하고, b 삭제 #정규분포를 따르는 W값을 정의한다. 

learning_rate = 0.001 #러닝메이트는 grad의 값을 얼 만큼 반영할 것인지 결정한다.
optimizer = tf.train.GradientDescentOptimizer(learning_rate) 

for i in range(1000+1): #1000+1번 수행하게 한다.
    with tf.GradientTape() as tape: #경사하강법을 구현하는 방법, 변수를 tape에 기록한다.
        hypothesis = tf.matmul(W, x_data) # b가 없다 #나의 가설 함수 
        cost = tf.reduce_mean(tf.square(hypothesis - y_data))#단순화 된 cost함수

    grads = tape.gradient(cost, [W]) #기울기(개별 미분값 W)를 구해서 grad로 구현한다. 순서대로 구현된다.
    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))
    if i % 50 == 0: #i값이 50의 배수가 될 때마다 프린트한다.
        print("{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}".format(
            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))#중간중간 50배수가  되는 값 프린트 한다
  
# Multi-variable linear regression (1)

X = tf.constant([[1., 2.], 
                 [3., 4.]]) #x1값의 데이터
y = tf.constant([[1.5], [3.5]]) #y1값의 데이터

W = tf.Variable(tf.random_normal([2, 1]))#정규분포를 따르는 W1값을 정의한다. 
b = tf.Variable(tf.random_normal([1]))#정규분포를 따르는 W1값을 정의한다. 

# Create an optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
        
tf.set_random_seed(0)  # for reproducibility #씨드 초기화
      
data = np.array([
     # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],   #각각 x1, x2, x3, y의 값
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)

# slice data
X = data[:, :-1]
y = data[:, [-1]]

W = tf.Variable(tf.random_normal([3, 1])) #정규분포를 따르는 W값을 정의한다. 
b = tf.Variable(tf.random_normal([1])) #정규분포를 따르는 b값을 정의한다. 

learning_rate = 0.000001 #러닝메이트는 grad의 값을 얼 만큼 반영할 것인지 결정한다.


# hypothesis, prediction function
def predict(X):
    return tf.matmul(X, W) + b #예측 가설

print("epoch | cost")

n_epochs = 2000
for i in range(n_epochs+1):
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad)# assign은 뺀 값을 다시 W에 할당한다. 값을 업데이트 시킨다.
    b.assign_sub(learning_rate * b_grad)# assign은 뺀 값을 다시 b에 할당한다. 값을 업데이트 시킨다.
    
    if i % 100 == 0:  #i값이 100의 배수가 될 때마다 프린트한다.
        print("{:5} | {:10.4f}".format(i, cost.numpy()))#중간중간 100배수가  되는 값 프린트 한다
        
W.numpy()
b.numpy()
tf.matmul(X, W) + b
Y # labels, 실제값
predict(X).numpy() # prediction, 예측값
# 새로운 데이터에 대한 예측
predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()
