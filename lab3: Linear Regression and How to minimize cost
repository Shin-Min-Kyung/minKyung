import tensorflow as tf #tensorflow 함수 부르기
import numpy as np #numpy함수 부르기

tf.enable_eager_execution()
tf.__version__#버전 확인하기

import numpy as np#numpy함수 부르기

X = np.array([1, 2, 3]) #X값의 데이터
Y = np.array([1, 2, 3]) #Y값의 데이터 

def cost_func(W, X, Y):  #X, Y에 대해서 W값이 주어졌을 때 cost를 계산한다.
    c = 0
    for i in range(len(X)):  
        c += (W * X[i] - Y[i]) ** 2 #cost함수의 식을 그대로 옯겨놓았으며 앞부분은 가설함수가 된다. 그래서 전체 부분이 오차 값이 된다.
    return c / len(X)# 위의 결과 값을 c로 누적해서 나타내어 평균을 낸 값이 뒤에 부분이다.

for feed_W in np.linspace(-3, 5, num=15): #시작과 끝을 지정하고 이 사이를 15개의 구간으로 나누며 이는 15개의 구간값을 가진다.
    curr_cost = cost_func(feed_W, X, Y) #cost 값 출력하기
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost)) 
    
X = np.array([1, 2, 3])#X값의 데이터
Y = np.array([1, 2, 3])#y값의 데이터

def cost_func(W, X, Y): #X, Y에 대해서 W값이 주어졌을 때 cost를 계산한다.
  hypothesis = X * W #나의 가설 함수
  return tf.reduce_mean(tf.square(hypothesis - Y))

W_values = np.linspace(-3, 5, num=15)
cost_values = []

for feed_W in W_values:
    curr_cost = cost_func(feed_W, X, Y)
    cost_values.append(curr_cost)
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))
    
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (8,6)

import matplotlib.pyplot as plt

plt.plot(W_values, cost_values, "b")
plt.ylabel('Cost(W)')
plt.xlabel('W')
plt.show()

tf.set_random_seed(0)  # for reproducibility#그래프 레벨의 난수 시드를 설정

x_data = [1., 2., 3., 4.]#X값의 데이터
y_data = [1., 3., 5., 7.]#y값의 데이터

W = tf.Variable(tf.random_normal([1], -100., 100.))#정규분포를 따르는 W값을 정의한다. 

for step in range(300): #300번 실행
    hypothesis = W * X #나의 가설 함수
    cost = tf.reduce_mean(tf.square(hypothesis - Y)) #단순화 된 cost 함수

    alpha = 0.01#alpha값 지정
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))#cost 함수를 미분한 식에서 W값은 포함이 안 되어 있다.
    descent = W - tf.multiply(alpha, gradient)# gradient에 위에 지정해준 alpha값을 곱해준 후 위에 식에서 W값을 빼준다. 이것이 새로운 W값  
    W.assign(descent)#W에 할당함으로써 업데이트 해준다.
    
    if step % 10 == 0:# i값이 10의 배수가 될 때마다 프린트한다.
        print('{:5} | {:10.4f} | {:10.6f}'.format(
            step, cost.numpy(), W.numpy()[0])) #중간중간 10배수가 되는 값 프린트 한 것

            
print(5.0 * W) #x에 5를 넣었을 때 실제로 5에 가까운지 확인한다.
print(2.5 * W) #x에 2.5를 넣었을 때 실제로 2.5에 가까운지 확인한다.

x_data = [1., 2., 3., 4.] #X값의 데이터
y_data = [1., 3., 5., 7.] #y값의 데이터


W = tf.Variable([5.0]) #W값을 특정한 값으로 지정한다.

for step in range(300): #300번 실행을 시킨다
    hypothesis = W * X #나의 가설
    cost = tf.reduce_mean(tf.square(hypothesis - Y)) #단순화한 cost함수

    alpha = 0.01  #alpha값 지정
    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))#코스트 함수를 미분한 식에서 W값은 포함이 안 되어 있다.
    descent = W - tf.multiply(alpha, gradient)# gradient에 위에 지정해준 alpha값을 곱해준 후 위에 식에서 W값을 빼준다. 이것이 새로운 W값 
    W.assign(descent)#W에 할당함으로써 업데이트 해준다.
    
    if step % 10 == 0:#10배수가 될 때마다 출력한다.
        print('{:5} | {:10.4f} | {:10.6f}'.format(
            step, cost.numpy(), W.numpy()[0]))#중간중간 출력된 cost갑과 w값을 프린트한다.
